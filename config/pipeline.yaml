# --- Paths -------------------------------------------------
data_dir:        data
video_dir:       data
model_dir:       models
workspace_root:  workspace

# --- Models & hardware ------------------------------------
tts_model_dir:   /models/ar_tortoise
rvc_model_path:  /models/rvc/voice_final.pth
gpu_index:       0
chunk_seconds:   20            # how the TTS splitter will chunk text

# --- RVC Configuration -----------------------------------
rvc:
  executable: cli/rvc_inference.py # our own RVC script (no third-party modification)
  model_path: models/rvc/jason.pth
  index_path: models/rvc/jason.index
  f0_up_key: 0                    # pitch shift in semitones
  f0_method: harvest              # harvest, pm, crepe, rmvpe
  index_rate: 0.66                # feature retrieval ratio
  device: cuda:2                  # cuda:2 for RTX 3090 performance
  is_half: true                   # use half precision
  filter_radius: 3                # harvest median filter radius
  resample_sr: 0                  # resample rate (0=no resample)
  rms_mix_rate: 1.0               # volume envelope mixing
  protect: 0.33                   # voiceless consonant protection
  crepe_hop_length: 128           # CREPE hop length parameter

# --- Audio Splicing Configuration ------------------------
audio_splicing:
  crossfade_duration: 0.1         # crossfade duration in seconds
  target_db: -20.0                # target audio level in dB
  normalize: true                 # normalize audio levels

# --- Video Analysis Configuration -------------------------
video_analysis:
  scene_threshold: 0.1            # scene detection sensitivity (0.0-1.0, lower = more sensitive for Keynote)
  movement_threshold: 0.1         # movement detection sensitivity (0.0-1.0)
  keynote_delay: 1.0              # keynote export delay compensation in seconds
  min_scene_duration: 0.5         # minimum duration between scenes in seconds
  validate_transitions: true      # validate against transcript transition cues
  output_dir: workspace/video_analysis  # directory for analysis manifests
  
  # Keynote-specific optimizations
  presentation_mode: true         # enable Keynote-specific scene detection algorithms
  enable_multi_algorithm: true    # use multiple detection methods for better accuracy
  enable_static_detection: true   # detect 1-second static periods before transitions
  enable_content_analysis: true   # histogram-based content change detection
  
  # Advanced tuning
  static_period_duration: 1.0     # duration for static period detection (seconds)
  content_threshold_multiplier: 1.5  # multiplier for content detection threshold
  merge_tolerance: 0.5            # time tolerance for merging similar detections (seconds)
  validation_tolerance: [0.7, 1.3]  # acceptable range for scene count validation

# --- Video Synchronization Configuration ------------------
video_sync:
  keynote_delay: 1.0              # keynote export delay compensation in seconds
  sync_tolerance: 0.1             # acceptable timing drift in seconds
  crossfade_handling: smooth      # how to handle audio crossfades (smooth, sharp)
  timing_validation: strict       # validation strictness level (strict, moderate, loose)
  output_format: mp4              # final video format
  quality_preset: medium          # ffmpeg encoding preset (ultrafast, fast, medium, slow, veryslow)
  audio_codec: aac                # audio codec for final output
  video_codec: copy               # video codec for final output (copy preserves original)
  audio_bitrate: 128k             # audio bitrate for final output
  sample_rate: 44100              # audio sample rate
  drift_correction: progressive   # drift correction method (progressive, global, none)
  validation_enabled: true        # enable sync quality validation
  preview_clips: false            # create preview clips for manual verification
  max_sync_offset: 0.5            # maximum allowed sync offset before error (seconds)

# --- Slide Synchronization Configuration ------------------
slide_sync:
  keynote_delay: 1.0              # keynote export delay compensation in seconds
  assembly_method: standard       # assembly method (standard, complex)
  optimize_gaps: true             # optimize gap types based on content analysis
  temp_dir: workspace/slide_sync  # temporary directory for processing
  gap_types:                      # gap filler configuration
    static_hold:                  # static frame hold
      enabled: true
      max_duration: 10.0          # maximum static hold duration in seconds
    animated_hold:                # animated loop hold  
      enabled: true
      loop_duration: 2.0          # duration of animation to loop
      max_loops: 5                # maximum number of loops
    fade_hold:                    # fade to static hold
      enabled: true
      fade_duration: 1.0          # fade transition duration
      fade_type: in               # fade type (in, out, inout)
  validation:
    max_gap_duration: 15.0        # maximum allowed gap duration
    max_timing_expansion: 100.0   # maximum timing expansion percentage
    warn_gap_threshold: 5.0       # gap duration to trigger warning
  output_settings:
    video_codec: libx264          # video codec for assembly
    audio_codec: aac              # audio codec for assembly  
    pixel_format: yuv420p         # pixel format
    crf: 23                       # constant rate factor for quality
    preset: medium                # encoding preset

# --- Pronunciation & QC -----------------------------------
pronun_csv:      config/pronunciations.csv
qc_threshold:    0.85          # cosine similarity score to accept a chunk

# --- Pipeline steps ---------------------------------------
steps:
  - id: check_datasets        # calls tasks.check_datasets()
  - id: extract_transcript    # calls tasks.extract_transcript()
    parameters:
      cue: "[transition]"
  - id: tts_run
    parameters:
      engine:                 orpheus # piper or orpheus
      piper_model:            en_GB-northern_english_male-medium
      orpheus_model:          canopylabs/orpheus-tts-0.1-finetune-prod
      orpheus_voice:          dan
      orpheus_temperature:    0.2
  - id: qc_pronounce
    substeps:
      - id: qc_pronounce_detect
        parameters:
          model: large-v3
        thresholds:
          wer: 0.10
          mos: 3.5
      - id: pronounce_fix
        parameters:
          max_attempts: 3
          phoneme_hint:
            piper: xsampa
            orpheus: ssml_ipa
  - id: apply_rvc
  - id: splice_audio
  - id: analyze_video
    parameters:
      scene_threshold: 0.1          # more sensitive for slide transitions
      movement_threshold: 0.1
      keynote_delay: 1.0
      validate_transitions: true
      presentation_mode: true       # enable Keynote-optimized detection
  - id: sync_slides
    parameters:
      assembly_method: standard
      optimize_gaps: true
      create_preview: false
  - id: make_srt
